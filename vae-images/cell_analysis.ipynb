{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from images.Metrics import calculate_metrics\n",
    "import numpy as np\n",
    "\n",
    "from model.VariationalAutoencoder import VariationalAutoencoder\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, save_img, img_to_array, array_to_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = os.environ.get('HOME') + '/model3/weights'\n",
    "FILE_MODEL = 'weights.h5'\n",
    "DATA_FOLDER = '/Volumes/My Passport/PFM/output/training_png'\n",
    "RUN_FOLDER =  '/Volumes/My Passport/PFM/run/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoencoder(image_folder=DATA_FOLDER, run_folder=RUN_FOLDER, train_mode= False, z_dim=200, use_dropout=False)\n",
    "vae.model.load_weights(os.path.join(MODEL_FOLDER, 'weights.h5'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.47058824\nimage format: PNG\nimage mode: RGB\nimage size: (128, 128)\nimage width: 128\nimage height: 128\nimage palette: None\nimage info: {}\nimage to array: (128, 128, 3)\nModel: \"model_22\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nencoder_input (InputLayer)      (None, 128, 128, 3)  0                                            \n__________________________________________________________________________________________________\nencoder_conv_1 (Conv2D)         (None, 64, 64, 32)   896         encoder_input[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_31 (BatchNo (None, 64, 64, 32)   128         encoder_conv_1[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_31 (LeakyReLU)      (None, 64, 64, 32)   0           batch_normalization_31[0][0]     \n__________________________________________________________________________________________________\nencoder_conv_2 (Conv2D)         (None, 32, 32, 64)   18496       leaky_re_lu_31[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_32 (BatchNo (None, 32, 32, 64)   256         encoder_conv_2[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_32 (LeakyReLU)      (None, 32, 32, 64)   0           batch_normalization_32[0][0]     \n__________________________________________________________________________________________________\nencoder_conv_3 (Conv2D)         (None, 16, 16, 64)   36928       leaky_re_lu_32[0][0]             \n__________________________________________________________________________________________________\nbatch_normalization_33 (BatchNo (None, 16, 16, 64)   256         encoder_conv_3[0][0]             \n__________________________________________________________________________________________________\nleaky_re_lu_33 (LeakyReLU)      (None, 16, 16, 64)   0           batch_normalization_33[0][0]     \n__________________________________________________________________________________________________\nflatten_6 (Flatten)             (None, 16384)        0           leaky_re_lu_33[0][0]             \n__________________________________________________________________________________________________\nmu (Dense)                      (None, 200)          3277000     flatten_6[0][0]                  \n__________________________________________________________________________________________________\nlog_var (Dense)                 (None, 200)          3277000     flatten_6[0][0]                  \n__________________________________________________________________________________________________\nencoder_output (Lambda)         (None, 200)          0           mu[0][0]                         \n                                                                 log_var[0][0]                    \n==================================================================================================\nTotal params: 6,610,960\nTrainable params: 6,610,640\nNon-trainable params: 320\n__________________________________________________________________________________________________\nimage to array after expand: (1, 128, 128, 3)\n(128, 128, 3)\n[[[ -5.6608725  -5.7490873  -6.3335   ]\n  [ -7.0841284  -7.1146264  -7.920987 ]\n  [ -7.640607   -7.665135   -8.495548 ]\n  ...\n  [-10.371618  -10.269055  -10.962632 ]\n  [ -9.560077   -9.616645  -10.272827 ]\n  [ -6.4452343  -6.843825   -7.122389 ]]\n\n [[ -6.989067   -7.118375   -8.029556 ]\n  [ -8.49084    -8.714613  -10.046551 ]\n  [ -9.122891   -9.027071  -10.3540325]\n  ...\n  [-12.250608  -12.2646885 -13.868391 ]\n  [-11.479465  -11.288126  -12.584692 ]\n  [ -7.673105   -7.9968     -8.70048  ]]\n\n [[ -7.1447477  -7.4257593  -8.056867 ]\n  [ -8.875839   -9.215376  -10.217163 ]\n  [ -9.793208   -9.575541  -10.9170685]\n  ...\n  [-12.521468  -12.35313   -13.664851 ]\n  [-12.063317  -11.504365  -12.803495 ]\n  [ -8.178528   -7.9252996  -9.060055 ]]\n\n ...\n\n [[ -9.163775   -9.003972  -10.236115 ]\n  [-11.316175  -11.11584   -13.087073 ]\n  [-12.288745  -11.562697  -13.719761 ]\n  ...\n  [-12.184146  -12.728965  -13.133668 ]\n  [-12.188975  -12.762605  -12.897287 ]\n  [ -8.507545   -9.404711   -9.448021 ]]\n\n [[ -8.272421   -8.257431   -9.139302 ]\n  [-10.522884  -10.541806  -11.926292 ]\n  [-11.781648  -11.2596445 -13.046706 ]\n  ...\n  [-12.153565  -13.060185  -12.762069 ]\n  [-12.153038  -13.14467   -12.621249 ]\n  [ -8.502092   -9.436859   -9.132641 ]]\n\n [[ -5.5710025  -5.676726   -6.3322515]\n  [ -7.007039   -7.0874147  -8.204161 ]\n  [ -7.689723   -7.2182226  -8.754845 ]\n  ...\n  [ -8.3287325  -8.987709   -9.277503 ]\n  [ -8.27007    -8.833073   -9.041508 ]\n  [ -5.9920125  -6.633255   -6.8454227]]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from PIL.ImageShow import show\n",
    "\n",
    "#image_path = '/Volumes/My Passport/PFM/output/training_png/Week9_39301_G11_4_Cells_34.png'\n",
    "image_path = '/Volumes/My Passport/PFM/output/training_png/Week5_28921_B02_2_Cells_13.png'\n",
    "#image_path = '/Volumes/My Passport/PFM/output/training_png/Week5_28921_B02_3_Cells_39.png'\n",
    "#image_path = '/Volumes/My Passport/PFM/output/training_png/Week7_34641_B02_1_Cells_28.png'\n",
    "\n",
    "img = Image.open(image_path)\n",
    "array_img  = img_to_array(img) # keras\n",
    "\n",
    "\n",
    "\n",
    "array_img = array_img / 255.\n",
    "\n",
    "print(\"max: \" + str(np.amax(array_img)))\n",
    "\n",
    "#print(array_img)\n",
    "\n",
    "img.show()\n",
    "\n",
    "print('image format: ' + str(img.format))\n",
    "#print('image shape: ' + str(img.shape))\n",
    "print('image mode: ' + img.mode)\n",
    "print('image size: ' + str(img.size))\n",
    "print('image width: ' + str(img.width))\n",
    "print('image height: ' + str(img.height))\n",
    "print('image palette: ' + str(img.palette))\n",
    "print('image info: ' + str(img.info))\n",
    "\n",
    "print('image to array: ' + str(array_img.shape))\n",
    "\n",
    "array_img = np.expand_dims(array_img, axis=0)\n",
    "\n",
    "\n",
    "vae.encoder.summary()\n",
    "\n",
    "print('image to array after expand: ' + str(array_img.shape))\n",
    "\n",
    "z_result = vae.encoder.predict(array_img)\n",
    "reco = vae.decoder.predict(z_result).squeeze()\n",
    "\n",
    "#print(reco)\n",
    "\n",
    "print(reco.shape)\n",
    "\n",
    "# scaled\n",
    "outimg = array_to_img(reco, scale=255.)\n",
    "\n",
    "show(outimg)\n",
    "\n",
    "print(reco)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(vae = vae, path = DATA_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
